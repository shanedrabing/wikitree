Imports

```{r, include=FALSE}
library(dplyr)
library(rvest)
library(shiny)
```

Constants

```{r}
RMGET_LIST <- NULL
PARSE_LIST <- NULL

PATTERN_TAXON <- "^Domain|Kingdom|Phylum|Class|Order|Family|Genus|Species"

FORMAT_GRAPH <- gsub("^\\s+", "", "
strict digraph {
layout=%s\noverlap=%s\nrankdir=%s
pad=0
node [shape=record style=filled fillcolor=white color=none]
%s
%s
}\n")

OPTIONS_LAYOUT <- c("dot", "neato", "twopi", "fdp")
OPTIONS_OVERLAP <- c("true", "false", "scale", "scalex", "scaley", "scalexy", "ortho")
OPTIONS_RANKDIR <- c("LR", "BT", "RL", "TB")
```

Request Manager

```{r}
rmget <- function(urls, dname) {
    # make the folder if it does not exist
    if (!dir.exists(dname)) {
        dir.create(dname, recursive = TRUE)
    }

    # format filepaths from input URLs
    fnames <- gsub("[^0-9A-z]", "", urls)
    fpaths <- sprintf("%s%s", file.path(dname, fnames), ".html")

    # filter URLs that don't have filepaths yet
    df <- data.frame(url = urls, fpath = fpaths)
    df_save <- df[!file.exists(fpaths), ]

    # write HTML to file
    if (nrow(df_save) != 0) {
        resps_save <- crul::Async$new(df_save$url)$get()
        for (i in 1:length(resps_save)) {
            content <- resps_save[[i]]$content
            fpath <- df_save$fpath[i]
            writeBin(content, fpath)
            RMGET_LIST[[fpath]] <<- rvest::read_html(content)
        }
    }

    # load from memory, or from file
    lapply(df$fpath, function(fpath) {
        if (fpath %in% names(RMGET_LIST)) {
            html <- RMGET_LIST[[fpath]]
        } else {
            html <- rvest::read_html(fpath)
            RMGET_LIST[[fpath]] <<- html
        }
    })
}


# TEST


terms <- c("horse", "spider", "cat", "Perissodactyla")
url <- sprintf("https://en.wikipedia.org/wiki/%s", terms)

# remove files (send request)
nil <- file.remove(file.path("rmget", dir("rmget")))
system.time(rmget(url, "rmget"))

# remove memory (load file)
RMGET_LIST <- NULL
system.time(rmget(url, "rmget"))

# load memory
system.time(htmls <- rmget(url, "rmget"))

summary(htmls)
```

Rvest Parser Components

```{r}
parse_h1 <- function(html) {
    html %>%
        html_node("h1") %>%
        html_text2()
}

parse_biota <- function(html) {
    html %>%
        html_node("table.biota")
}

parse_biota_th <- function(biota) {
    biota %>%
        html_node("th") %>%
        html_text2() %>%
        gsub("\\n.+$", "", .)
}

parse_biota_img <- function(biota) {
    biota %>%
        html_node("img") %>%
        html_attr("src") %>%
        gsub("^//", "https://", .) %>%
        gsub("^/", "https://en.wikipedia.org/", .)
}

parse_biota_td <- function(biota) {
    mat <- biota %>%
        html_nodes("tr") %>%
        lapply(function(x) {
            td <- html_nodes(x, "td")
            if (length(td) != 2) {
                return()
            }
            text <- html_text2(td)
            if (!endsWith(text[1], ":")) {
                return()
            }
            text <- stringr::str_extract(text, "^([^:\\[](?:\\.|-)*)+(?:\\b|\\s)*?")
            text <- stringr::str_extract(text, "^([^\\n])+")
            text
        }) %>%
        do.call(what = rbind)
 
    if (1 < nrow(mat)) {
        for (i in 2:nrow(mat)) {
            abb <- gsub("\\b([A-z])\\w+?\\b", "\\1\\\\.", mat[i - 1, 2])
            mat[i, 2] <- gsub(abb, mat[i - 1, 2], mat[i, 2])
        }
    }

    apply(mat, 1, toString)
}


# TEST


html <- htmls[[4]]
parse_h1(html)

biota <- parse_biota(html)
parse_biota_th(biota)
parse_biota_img(biota)
parse_biota_td(biota)
```

Rvest Parser

```{r}
parse <- function(html) {
    biota <- parse_biota(html)
    if (is.na(biota)) {
        return()
    }

    list(
        h1 = parse_h1(html),
        th = parse_biota_th(biota),
        img = parse_biota_img(biota),
        td = parse_biota_td(biota)
    )
}


# TEST


system.time(lst <- lapply(htmls, parse))
summary(lst)
```

Scrape and Parse

```{r}
scrape_and_parse <- function(terms, dname, crawl = 1) {
    for (epoch in 1:crawl) {
        if (epoch != 1) {
            new <- do.call(c, lapply(PARSE_LIST[terms], function(lst) {
                gsub("^.+, ", "", lst$td)
            }))
            terms <- union(terms, new)
        }
    
        index <- (terms %in% names(PARSE_LIST))
        terms_sel <- terms[!index]
    
        if (!identical(terms_sel, character(0))) {
            urls <- sprintf(
                "https://en.wikipedia.org/w/index.php?search=%s",
                gsub("\\s+", "%20", terms_sel)
            )

            htmls <- rmget(urls, dname)
            biota <- lapply(htmls, parse)
            for (i in 1:length(terms_sel)) {
                if (is.null(biota[[i]])) {
                    next
                }
                PARSE_LIST[[terms_sel[i]]] <<- biota[[i]]
            }
        }
    }
    
    snp <- PARSE_LIST[terms]
    snp[!is.na(names(snp))]
}


# TEST


# params
terms <- c("horse", "spider", "cat")
dname <- "rmget"

# remove files (send request)
nil <- file.remove(file.path(dname, dir(dname)))
RMGET_LIST <- NULL
PARSE_LIST <- NULL
system.time(scrape_and_parse(terms, dname))

# remove memory (load file)
RMGET_LIST <- NULL
PARSE_LIST <- NULL
system.time(scrape_and_parse(terms, dname))

# remove memory
PARSE_LIST <- NULL
system.time(scrape_and_parse(terms, dname))

# load memory
system.time(snp <- scrape_and_parse(terms, dname))

summary(snp)
```

Crawl Test

```{r}
terms <- c("horse", "spider", "cat")
dname <- "rmget"

system.time(snp <- scrape_and_parse(terms, dname, crawl = 3))
summary(snp)
```

Simplify Network

```{r}
simplify_network <- function(snp, terms) {
    snp_sel <- snp[terms]

    last <- vapply(snp_sel, function(lst) {
        tail(lst$td, 1)
    }, character(1))

    common <- NULL
    for (i in 1:(length(snp_sel) - 1)) {
        for (j in (i + 1):length(snp_sel)) {
            new <- tail(intersect(snp_sel[[i]]$td, snp_sel[[j]]$td), 1)
            if (is.null(new) || identical(new, character(0))) {
                next
            }
            common[[new]] <- new
        }
    }

    lapply(snp, function(lst) {
        if (any(lst$td %in% last)) {
            return(lst)
        }

        is_last <- (lst$td %in% last)
        is_comm <- (lst$td %in% common)
        is_dkpc <- grepl(PATTERN_TAXON, lst$td)
        lst$td <- lst$td[is_last | is_comm | is_dkpc]
        lst
    })
}


# TEST


terms <- c("horse", "spider", "cat")
dname <- "rmget"

snp <- scrape_and_parse(terms, dname)
snp <- simplify_network(snp, terms)

summary(snp)
```

Format Graph

```{r}
slicer <- function(vec, start=NULL, end=NULL) {
    if (is.null(start)) { start <- 1 }
    if (is.null(end)) { end <- length(vec) }
    while (start < 0) { start <- start + length(vec) }
    while (end < 0) { end <- end + length(vec) }
    vec[start:end]
}

form_nodes <- function(snp) {
    do.call(c, lapply(snp, function(lst) {
        c(
            sprintf(
                '"%s" [label=< <i>%s</i> > fillcolor=skyblue]',
                head(lst$td, -1), gsub("^.+, ", "", head(lst$td, -1))
            ),
            sprintf(
                '"%s" [label=< <b>%s</b><br/><i>%s</i> > fillcolor=dodgerblue]',
                tail(lst$td, 1), lst$th, gsub("^.+, ", "", tail(lst$td, 1))
            )
        )
    }))
}

form_edges <- function(snp) {
    mat_ind <- do.call(rbind, lapply(snp, function(lst) {
        if (1 < length(lst$td)) {
            new <- cbind(slicer(lst$td, end = -1), slicer(lst$td, 2))
            cbind(new, c(rep(0, nrow(new) - 1), 1))
        }
    }))

    mat <- mat_ind[, 1:2]
    terms <- mat[mat_ind[, 3] == "1", 2]

    new_mat <- NULL
    for (term in terms) {

        lst <- NULL
        new_lst <- NULL
        lst[[term]] <- term
        while (!identical(lst, new_lst)) {
            new_lst <- lst

            for (key in names(lst)) {
                path <- lst[[key]]
    
                child <- head(path, 1)
                parents <- mat[mat[, 2] == child, 1]
                if (identical(parents, character(0))) {
                    next
                }

                lst[[key]] <- NULL
                for (parent in parents) {
                    lst[[parent]] <- c(parent, path)
                }
            }
        }

        lengths <- vapply(lst, length, numeric(1))
        longest <- which(lengths == max(lengths))[1]
        
        vec <- lst[[longest]]
        
        new <- cbind(slicer(vec, end = -1), slicer(vec, 2))
        new_mat <- rbind(new_mat, new)
    }
    
    sprintf('"%s"->"%s"', new_mat[, 1], new_mat[, 2])
}

join <- function(..., sep = "", collapse = "") {
    paste(..., sep = sep, collapse = collapse)
}

form_graph <- function(
        terms, dname,
        crawl = 1, simplify = TRUE,
        layout = OPTIONS_LAYOUT,
        overlap = OPTIONS_OVERLAP,
        rankdir = OPTIONS_RANKDIR) {

    simplify <- as.logical(simplify)
    layout <- match.arg(layout)
    overlap <- match.arg(overlap)
    rankdir <- match.arg(rankdir)

    snp <- scrape_and_parse(terms, dname, crawl)
    if (simplify) {
        snp <- simplify_network(snp, terms)
    }

    sprintf(
        FORMAT_GRAPH,
        layout, overlap, rankdir,
        join(form_nodes(snp), collapse = "\n"),
        join(form_edges(snp), collapse = "\n")
    )
}


# TEST


terms <- c("horse", "spider", "cat")
dname <- "rmget"

graph <- form_graph(terms, dname, layout = "neato", overlap = "scale")
DiagrammeR::grViz(graph)

graph <- form_graph(terms, dname)
DiagrammeR::grViz(graph)
```

Crawl Graph

```{r}
terms <- c("horse", "spider", "cat")
dname <- "rmget"

DiagrammeR::grViz(form_graph(terms, dname, crawl = 1))
DiagrammeR::grViz(form_graph(terms, dname, crawl = 2))
DiagrammeR::grViz(form_graph(terms, dname, crawl = 3))
DiagrammeR::grViz(form_graph(terms, dname, crawl = 4))
DiagrammeR::grViz(form_graph(terms, dname, crawl = 5))
DiagrammeR::grViz(form_graph(terms, dname, crawl = 6))
DiagrammeR::grViz(form_graph(terms, dname, crawl = 7))
```

Penguin & Dinosaur

```{r}
DiagrammeR::grViz(form_graph(c("penguin", "T rex"), "rmget", crawl = 10))
```


